# OptAzur: Optimization in French Riviera

OptAzur is an ongoing effort to foster collaborations among members of Université Côte d'Azur on different aspect of optimization and its applications to machine learning, imaging and signal processing, etc. 

## Seminar

OptAzur organizes a monthly seminar in Nice and Sophia-Antipolis, which alternates between the two sites and takes place on the third Monday of each month. [Google Calendar](https://calendar.google.com/calendar/u/0?cid=Nzc3NjM0ZDhlMjNkMjE2YTIyZjJlNDVkMmYxYzU2Y2ZkMWIyY2FmZDRkZWRiMGY0ODQ1OGE1NWJlZjRmN2EwZkBncm91cC5jYWxlbmRhci5nb29nbGUuY29t)

### Next talk

Monday, September 18th, 2023 (Salle Fizeau, LJAD, Nice)

14h - [Jean-François Aujol](https://www.math.u-bordeaux.fr/~jaujol/) (Université de Bordeaux)

**FISTA is an automatic geometrically optimized algorithm for strongly convex functions**

This work is related with large scale optimization. We are interested in the famous FISTA algorithm. We show that FISTA is an automatic geometrically optimized algorithm for functions satisfying a quadratic growth assumption. This explains why FISTA works better than the standard Forward-Backward algorithm (FB) in such a case, although FISTA is known to have a polynomial asymptotical convergence rate while FB is exponential. We provide a simple rule to tune the α parameter within the FISTA algorithm to reach an ε-solution with an optimal number of iterations. These new results highlight the efficiency of FISTA algorithms, and they rely on new non asymptotic bounds for FISTA.
This is a joint work with Charles Dossal and Aude Rondepierre (INSA Toulouse).

15h15 - [Luca Calatroni](https://sites.google.com/view/lucacalatroni/home) (CNRS, I3S)

**Scaled, inexact and adaptive FISTA for (strongly) convex optimisation**

We design an inexact, scaled and generalised Fast Iterative Soft-Thresholding Algorithm (FISTA) for minimising the sum of two (possibly strongly) convex functions. Inexactness is explicitly taken into account for describing situations where proximal operators cannot be evaluated in closed form. The use of data-dependent scaling allows to incorporate Newton-type information along the optimisation via variable-metric updates. Finally, non-monotone backtracking is used to improve convergence speed.  Linear convergence for the function values is proved with rates depending on backtracking/scaling and strong convexity parameters. The performance of the proposed algorithm, named SAGE-FISTA, is validated on exemplar imaging problems where sparsity-promoting (l_1, TV) regularisation is combined with popular data-fidelity terms. Numerical results show improved performance and practical efficiency under limited computational budget.

### Previous talks

## Events

The members organize several conferences and workshop relevant to the optimization community, as detailled below.

- [Bilevel optimization in machine learning and imaging sciences workshop](https://iciam2023.org/registered_data?id=00400) @[ICIAM 2023](https://iciam2023.org/accepted_ms#00400_Bilevel_optimization_in_machine_learning_and_imaging_sciences), Tokyo, Japan. (Organizers: L. Calatroni, S. Vaiter)

- [Optimal control: methods and applications](https://iciam2023.org/registered_data?id=00731) @[ICIAM 2023](https://iciam2023.org), Tokyo, Japan. (Organizers: J.-B. Caillau, L. Dell'Elce, Clément Moreau)

## Scientific Committee

- [Laure Blanc-Féraud](https://www.i3s.unice.fr/~blancf/)
- [Luca Calatroni](https://sites.google.com/view/lucacalatroni/home) (co-organizer)
- [Jean-Baptiste Caillau](https://caillau.perso.math.cnrs.fr)
- [Yassine Laguel](https://yassine-laguel.github.io)
- [Samuel Vaiter](https://samuelvaiter.com) (co-organizer)
