# OptAzur: Optimization in French Riviera

OptAzur is an ongoing effort to foster collaborations among members of Université Côte d'Azur on different aspect of optimization and its applications to machine learning, imaging and signal processing, etc. 

## Seminar

OptAzur organizes a monthly seminar in Nice and Sophia-Antipolis, which alternates between the two sites and takes place on the third Monday of each month.

[Indico Calendar](https://indico.math.cnrs.fr/category/674) (and [subscription](https://indico.math.cnrs.fr/category/674/events.ics?user_token=5028_H1YM2-rmITmgW7O10-SgrDIW0sQWLtM6EhJMmJy1u5A))

### Next talk

Monday, April 15th, 2024 (LJAD, Nice)

14h - [Lorenzo Rosasco](https://web.mit.edu/lrosasco/www/) (University of Genova and MIT)

**Being a kernel in the age of deep learning**

Reproducing Kernel Hilbert Spaces (RKHS) have been a cornerstone in disciplines ranging from quantum physics to signal processing since their inception in the 1950s. Their integration into statistics in the 1970s and their explosion onto the machine learning scene in the early 2000s were driven by their unusual combination of practical performance and elegant theory. However, the rise of deep learning in the early 21st century has shifted the spotlight, leading to questions about the current role of kernel methods in machine learning.
I will develop the discussion along three lines. Firstly, I will illustrate how enhancements in scalability could still make kernel methods useful tools especially when efficiency is mandatory.  Secondly, I will discuss the potential of kernel methods when learning  problems characterized by structural equations, such as  dynamical systems and partial differential equations. Lastly, I will delve into the distinctive features of RKHS in contrast to neural networks and discuss how the concept of Reproducing Kernel Banach Spaces may offer insights on the latter. 

15h15 - [Lénaïc Chizat](https://lchizat.github.io) (EPFL)

**A Formula for Feature Learning in Large Neural Networks**

Deep learning succeeds by doing hierarchical feature learning, but tuning hyperparameters such as initialization scales, learning rates, etc., only give indirect control over this behavior. This calls for theoretical tools to predict, measure and control feature learning.  In this talk, we will first review various theoretical advances (signal propagation, infinite width dynamics, etc) that have led to a better understanding of the subtle impact of hyperparameters and architectural choices on the training dynamics. We will then introduce a formula which, in any architecture, quantifies feature learning in terms of more tractable quantities: statistics of the forward and backward passes, and a notion of alignment between the feature updates and the backward pass which captures an important aspect of the nature of feature learning. This formula suggests normalization rules for the forward and backward passes and for the layer-wise learning rates. To illustrate these ideas, I will discuss the feature learning behavior of ReLU MLPs and ResNets in the infinite width and depth limit. 

Talk based on: https://arxiv.org/abs/2311.18718 (joint work with Praneeth Netrapalli)

### Previous talks

Titles and abstracts [here](/previous)

- #1: [Jean-François Aujol](https://www.math.u-bordeaux.fr/~jaujol/) (Université de Bordeaux) and [Luca Calatroni](https://sites.google.com/view/lucacalatroni/home) (CNRS, I3S)
- #2: [Gersende Fort](https://perso.math.univ-toulouse.fr/gfort/) (CNRS, Institut de Mathématiques de Toulouse) and [Samuel Vaiter](https://samuelvaiter.com) (CNRS, Laboratoire J. A. Dieudonné)
- #3: [Massimiliano Pontil](https://www.iit.it/people-details/-/people/massimiliano-pontil) (Italian Institute of Technology and University College London) and [Mathieu Carrière](https://www-sop.inria.fr/members/Mathieu.Carriere/) (Inria)
- #4: [Maurizio Filippone](https://www.eurecom.fr/~filippon/) (EURECOM) and [Yassine Laguel](https://yassine-laguel.github.io) (Laboratoire J. A. Dieudonné)
- #5: [Marco Lorenzi](https://marcolorenzi.github.io) (Inria)
- #6: [Juan Peypouquet](https://www.rug.nl/staff/j.g.peypouquet/?lang=en) (University of Groningen) and [Khazhgali Kozhasov](https://scholar.google.com/citations?user=cWl9pB0AAAAJ) (LJAD, Nice)
- #7: [Aris Daniilidis](https://www.arisdaniilidis.at) (VADOR, TU Wien) and [Wellington de Oliveira](https://www.oliveira.mat.br) (Mines Paris)

## Events

The members organize several conferences and workshops relevant to the optimization community, as detailled below.

- [Bilevel optimization in machine learning and imaging sciences workshop](https://iciam2023.org/registered_data?id=00400) @[ICIAM 2023](https://iciam2023.org/accepted_ms#00400_Bilevel_optimization_in_machine_learning_and_imaging_sciences), Tokyo, Japan. (Organizers: L. Calatroni, S. Vaiter)

- [Optimal control: methods and applications](https://iciam2023.org/registered_data?id=00731) @[ICIAM 2023](https://iciam2023.org), Tokyo, Japan. (Organizers: J.-B. Caillau, L. Dell'Elce, Clément Moreau)

## Scientific Committee

- [Laure Blanc-Féraud](https://www.i3s.unice.fr/~blancf/)
- [Luca Calatroni](https://sites.google.com/view/lucacalatroni/home) (co-organizer)
- [Jean-Baptiste Caillau](https://caillau.perso.math.cnrs.fr)
- [Yassine Laguel](https://yassine-laguel.github.io)
- [Samuel Vaiter](https://samuelvaiter.com) (co-organizer)
